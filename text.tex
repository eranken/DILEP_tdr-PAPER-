%% main text
\section{Introduction}
\label{S:intro}

The standard model predicts that the top quark, like other massive fermions,  acquires its mass, $m_\mathrm{t}$, through its Yukawa coupling to the Higgs field, $g_\mathrm{t}=m_\mathrm{t}/\sqrt{2}v$, where $v$ is the vacuum expectation value of the Higgs field. Among all such couplings, the top quark Yukawa coupling  is of particular interest. It is not only the largest, but also suspiciously close to unity. This makes the coupling both theoretically intriguing and experimentally accessible to measurement. Recent efforts have seen the first success in directly constraining $g_\mathrm{t}$ \cite{ttH}. However, it is also possible to constrain $g_\mathrm{t}$ indirectly using the kinematic distributions of top and antitop quarks produced in hadronic collisions, as done recently in \cite{ytpaper}. This paper follows closely the latter approach, which we will briefly outline, and adapts it to a different \ttbar decay channel.

Current Monte Carlo (MC) simulations of \ttbar production include next-to-leading order (NLO) precision in perturbative Quantum Chromodynamics (QCD). Sub-leading-order corrections arise from including electroweak (EW) terms in a perturbative expansion of $\alpha_S$ and $\alpha_\mathrm{EW}$, the latter of which is much smaller and traditionally shortened to $\alpha$. Such terms begin to noticeably affect the cross-section only at loop-induced order $\alpha_s^2 \alpha$, and are typically ignored. While these have a very small effect on the total cross-section, they can alter the shape of kinematic distributions to a measurable extent. Such changes will be more noticeable if the Yukawa coupling affecting the loop correction (Figure \ref{fig:diagrams}) is anomalously large. Thus, these corrections are of particular interest in placing upper limits on $g_t$. For example, the distribution of the invariant mass of the \ttbar system, $M_{\ttb}$, will be affected significantly by varying $g_\mathrm{t}$. In particular, doubling the value of $g_\mathrm{t}$ can alter the $M_{\ttb}$ distribution by about 9\% near the production threshold as described in \cite{Uwer} and shown in Figure \ref{fig:hatratgen}(left). Another variable sensitive to the value of $g_\mathrm{t}$ is the difference in rapidity between top and anti top quarks $\Delta y_{\ttb} = y(\mathrm{t})-y(\mathrm{\bar{t})}$ , shown in Figure \ref{fig:hatratgen}(right). Together, these two variables form a basis for a 2D differential cross section of \ttbar decays, and can be used to include the EW corrections on arbitrary samples via weights.
\begin{figure}
    \centering
    \includegraphics[width=.85\linewidth]{figs/quarkgluon.png}
    \caption{Sample diagrams for EW contributions to gluon-induced and quark-induced top pair production, where $\Gamma$ stands for neutral gauge bosons, Higgs boson and pseudo-Goldstone bosons. The correction due to the interference of these diagrams with the LO QCD diagrams is evaluated using HATHOR package.}
    \label{fig:diagrams}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.48\linewidth]{figs/ratMttgen.pdf}
    \includegraphics[width=.48\linewidth]{figs/ratDyttgen.pdf}
    \caption{Effect of the EW corrections on \ttbar differential kinematic distributions for different values of $\yt$, after reweighting of gen-level events from Powheg+Pythia8.}
    \label{fig:hatratgen}
\end{figure}




Thus, the effect of EW corrections can be considered without generating new MC samples. This is achieved by applying  weight corrections to the current standard \ttbar MC samples used for top physics analysis by the CMS experiment. Contributions to the top quark pair production due to the interference of QCD+EW diagrams, shown in Figure \ref{fig:diagrams},  with the leading order (LO) QCD diagrams are evaluated using HATHOR (v2.1) package~\cite{hathorart}. The ratio of these contributions to the LO QCD production is applied as parton-level weight corrections to events simulated using POWHEG+PYTHIA8, which incorporates fully NLO QCD calculations and parton shower effects \cite{Nason:2004rx,Frixione:2007vw,Alioli:2010xd,Campbell:2014kua,Sjostrand:2006za,Sjostrand:2007gs} \cite{Frederix:2012ps}. In HATHOR, we can control the ratio of the Yukawa coupling used in computing the Electroweak corrections to the standard model value, 
\begin{equation}
\yt = g_\mathrm{t}\mathrm{(HATHOR)}/g_\mathrm{t}\mathrm{(SM).} 
\end{equation}

Weights are calculated for discrete integer values of $\yt=0,1,\ldots,5.$ After event reconstruction, data from the discrete values is interpolated to generate a continuous template as a function of $\yt$. Using this parameterization we are able to perform a maximum likelihood fit and obtain an estimate for $\yt$ based on the kinematic distributions observed in data. 

Our aim is to take this approach, which has been successfully applied to \ttbar decays in the lepton+jets channel \cite{ytpaper}, and apply it to the dilepton channel. While this channel has a smaller branching ratio, it has much lower backgrounds due to the distinguishing presence of two final state leptons. However, it also contains two final state neutrinos, presenting the question of whether it is necessary to perform a full kinematic reconstruction with two missing final state particles. 

In this note, we will discuss first our Data/MC samples (Section \ref{S:data}), followed by our methods for event selection (Section \ref{S:selection}) and reconstruction (Section \ref{S:reco}). We then present an outline of the measurement technique (Section \ref{S:stats}), and conclude with a preliminary sensitivity study done on the Asimov dataset, demonstrating the current expected sensitivity of our measurement (Section \ref{S:fit}). 
Uncertainty sources are described in detail in Appendix \ref{S:uncs}.

\clearpage


\section{The CMS detector}
\label{sec:detector}
The central feature of the CMS detector is a superconducting solenoid of 6m internal diameter,
providing a magnetic field of 3.8 T. Within the solenoid volume are a silicon pixel and strip
tracker, a lead tungstate crystal electromagnetic calorimeter (ECAL), and a brass and scintillator
hadron calorimeter (HCAL), each composed of a barrel and two endcap sections. Forward calorimeters
extend the coverage provided by the barrel and endcap detectors. Muons
are measured in gas-ionization detectors embedded in the steel flux-return yoke outside the
solenoid. 

The particle-flow (PF) algorithm~\cite{CMS-PRF-14-001} aims to reconstruct and identify each individual particle in an event, with an optimized combination of information from the various elements of the CMS detector. The energy of photons is obtained from the ECAL measurement. The energy of electrons is determined from a combination of the electron momentum at the primary interaction vertex as determined by the tracker, the energy of the corresponding ECAL cluster, and the energy sum of all bremsstrahlung photons spatially compatible with originating from the electron track. The energy of muons is obtained from the curvature of the corresponding track. The energy of charged hadrons is determined from a combination of their momentum measured in the tracker and the matching ECAL and HCAL energy deposits, corrected for zero-suppression effects and for the response function of the calorimeters to hadronic showers. Finally, the energy of neutral hadrons is obtained from the corresponding corrected ECAL and HCAL energies.A more detailed description of the CMS detector, together with a definition of the
coordinate system and relevant kinematical variables, can be found in Ref. ~\cite{Chatrchyan:2008zzk}.

\section{Data and Monte Carlo (MC) Samples}



We utilize the full CMS Run 2 dataset. As the \ttbar dilepton channel  overall has very low background level, we rely on Monte Carlo simulation to estimate their rates and kinematic distributions. In particular, we  account for dilepton production due to Drell-Yan type processes and single top production. Backgrounds from QCD multijet production were found to be negligible and are not included. Among vector+jets backgrounds, W decay samples were also investigated and found to have negligible contributions. 

Around 1\% of events identified as \ttbar dilepton are in fact misidentified \ttbar lepton+jets events.  EW corrections are applied to all \ttbar events,  even  misidentified ones. This approach also allows us to apply systematic uncertainty variations to the \ttbar sample uniformly.

Single top events are simulated with Powheg+Phythia8, while Drell-Yan decays are simulated using the MadGraph5 generator \cite{Alwall:2014hca}. Expected signal and background counts, after event selection, are discussed in Section \ref{SS:bg}.

\label{S:data}

\clearpage
\section{Event Selection}
\label{S:selection}

\subsection{Trigger}
\label{SS:trigger}
 Events are selected from data based on single lepton triggers. based on availability of different $p_T$ triggers, in the muon channel we use a $p_T$ threshold of 24/27/24 GeV  in 2016/2017/2018 datasets respectively, and for electrons a $p_T$ threshold of 27/27/32 in 2016/2017/2018. To avoid a double counting of events, all events selected by the chosen muon triggers are rejected in the SingleElectron/EGamma data stream. 


\subsection{Leptons}
Due to the use of single-lepton triggers, we require that at least one lepton pass a leading $p_T$ selection above the online trigger threshold. We then require that an additional lepton of opposite sign passes a less stringent sub-leading $p_T$ selection. We reject events with more than 2 leptons passing the sub-leading $p_T$ selection.

\subsection*{Muons}
The pseudo-rapidity of the selected muons is restricted to the silicon tracker coverage of $\abs{\eta} < 2.4$. A cut-based muon selection is used for identification, with isolation required. The leading muon $p_T$ is requires to be greater than $30$\,GeV. This ensures that the offline selection is well above the trigger threshold of 27\,GeV. For the sub-leading muons, we require  $p_T>15$\,GeV.

Efficiencies of reconstruction, identification and the trigger have been studied and scale factors are applied to the simulation based on a tag and probe method applied to leptons originating from $Z$ boson decay. The corresponding systematic uncertainty is  described in Appendix \ref{S:otherunc}.

\subsection*{Electrons}
 We restrict the pseudo-rapidity of the selected electrons to the silicon tracker coverage of $\abs{\eta} < 2.4$. Electrons with an ECal super cluster (SC) in the gap between the barrel and the endcap, $1.442 < \eta(SC) < 1.566$, are rejected. A cut-based ID is used, which includes an isolation requirement similar to muons. In addition, a selection on the transverse impact parameter $\abs{d_\mathrm{0}} < 0.05(0.10)$\,cm and the longitudinal impact parameter $\abs{d_\mathrm{z}} < 0.1(0.2)$\,cm is applied in the barrel(endcap) regions.
 
 In 2016 and 2017 datasets the trigger $p_T$ threshold for isolated electrons is 27\,GeV, although there were periods in 2017, when the trigger based on electrons with  $p_T$ below  32\,GeV was pre-scaled. For both 2016 and 2017 we choose to use an offline selection of  $p_T > 30$\,GeV, and to treat the pre-scales in 2017 as an additional inefficiency. In 2018 the  trigger threshold was raised to 32\,GeV and an offline leading $p_T$ cut of $p_T>34$\,GeV is used. In all years, the sub-leading electron requirement is only $p_T>15$\,GeV.

 Scale factors are generated, applied, and considered as uncertainty sources following the same methods used for muons.

\subsection{Jets and b-jet identification}
The default jets are clustered from PF objects using the anti-$k_T$ jet algorithm with a distance parameter of 0.4. Charged particles originating from a pileup interaction vertex are excluded. 

The DeepCSV b-tagging algorithm  is used for the identification of b-jets \cite{deepCSVref}. We require at least two b-jets to pass the loose working point selection criterion. Scale factors derived on data are used to correct the tagging and mistagging efficiencies. Uncertainty on these efficiencies is discussed in Appendix \ref{S:otherunc}.

 In events with more than two b-tagged jets, the jets are sorted by the CSV discriminant, with the two leading-CSV jets assumed to be the most likely candidates for b-jets originating from a top decay. After sorting, if the third jet has a similar CSV value to the second (difference $<0.25$), the event is discarded as it is considered unclear which b-jets to use. This is a rare enough occurrence that it is not clear whether it would be worthwhile to invest more effort in selecting 2 b-jets from 3 or more candidates. 
 
 We find that in \ttbar events, the two b-jets are among the candidates passing the loose DeepCSV working point in roughly 93\% of events. In cases where a collection of 3 or more candidates does include the correct b-jets, our simple CSV-sort and cut yields a \ttbar sample where the two b-jets are correctly chosen in 89\% of events.
   


\subsection*{Drell-Yan background removal}

In order to remove Drell-Yan background resulting from $Z\ \gamma \rightarrow e^+e^- $ or $Z \ \gamma \rightarrow \mu^+\mu^-$, we apply some additional selection criteria in the $ee$ and $\mu\mu$ channels. We require that the invariant mass of the two leptons, $M_{l\bar{l}}$ is not within $10$ \unit{GeV} of the $Z$-boson mass, 91.2 \unit{GeV}. Additionally, we require $M_{l\bar{l}}$  above 50\,GeV and in order to match the Monte Carlo sample used for Drell-Yan background modelling. 

Additionally, we apply selection based on a missing transverse momentum, MET, of $p_{T}\mathrm{,miss}>30$ \unit{GeV}. These selection criteria remove the majority of Drell-Yan background. 

\subsection{Additional Corrections}
\par \noindent
\textbf{Pileup }

Pileup reweighting is applied to adjust the number of interactions generated in the Monte Carlo to match the data. A corresponding uncertainty is included as described in Appendix \ref{S:otherunc}.



\subsection{Event yields \& background estimation}
\label{SS:bg}

The breakdown of expected signal and background counts is shown by year in Table \ref{tab:eventcounts}. The vector+jets background, which originates almost entirely from Drell-Yan same-flavor lepton production, is estimated to be reduced to around 2\%. Decays from single top production account for roughly another 2\% of the estimated sample composition. Despite some differences in kinematic distributions in the $e\mu$ channel and the same-flavor channels, where Drell-Yan suppression selection is applied, there was no clear benefit in separating the decay channels by lepton flavor.


\begin{table}[]
\centering

\begin{tabular}{ l | c | c }
2016 & yield ($\pm \sigma_\mathrm{stat}$) & \% of total MC \\ \hline
\ttbar\;signal MC & 139617\;$\pm$\;164.7 & 96.6\% \\ 
vector + jets MC & 1947\;$\pm$\;53.7 & 1.3\% \\ 
single top MC & 3013\;$\pm$\;25.3 & 2.1\% \\ 
\hline total MC  & 144578\;$\pm$\;175.0 & 100\% \\ 
\hline data  & 144188 & 99.7\% \\ 
\end{tabular}

\vskip.6cm
\begin{tabular}{ l | c | c }
2017 & yield ($\pm \sigma_\mathrm{stat}$) & \% of total MC \\ \hline
\ttbar\;signal MC & 168778\;$\pm$\;94.4 & 95.6\% \\ 
\ttbar\;background MC & 1052\;$\pm$\;12.1 & 0.6\% \\ 
vector + jets MC & 3214\;$\pm$\;75.2 & 1.8\% \\ 
single top MC & 3513\;$\pm$\;24.0 & 2.0\% \\ 
\hline total MC  & 176558\;$\pm$\;123.7 & 100\% \\ 
\hline data  & 177182 & 100.4\% \\ 
\end{tabular}
\vskip.6cm
\begin{tabular}{ l | c | c }
2018 & yield ($\pm \sigma_\mathrm{stat}$) & \% of total MC \\ \hline
\ttbar\;signal MC & 242689\;$\pm$\;140.0 & 95.7\% \\ 
\ttbar\;background MC & 1416\;$\pm$\;17.4 & 0.6\% \\ 
vector + jets MC & 4624\;$\pm$\;128.1 & 1.8\% \\ 
single top MC & 4907\;$\pm$\;26.9 & 1.9\% \\ 
\hline total MC  & 253636\;$\pm$\;192.4 & 100\% \\ 
\hline data  & 261507 & 103.1\% \\ 
\end{tabular}





    \caption{MC and data event yields for all 3 years. Statistical uncertainty on simulated event counts is given. For estimates of systematic uncertainty, see Figures \ref{fig:dmcpre_16}--\ref{fig:dmcpost_18}.}
    \label{tab:eventcounts}
\end{table}



\clearpage
\section{Event Reconstruction}
\label{S:reco}

\begin{figure}
    \centering
    \includegraphics[width=.5\linewidth]{figs/ttbar3.png}
    \caption{Diagram of a \ttbar decay in the dilepton channel. The process can be induced by quark or gluon processes at the LHC.}
    \label{fig:ttdia}
\end{figure}

EW corrections are evaluated as a function of $M_{\ttbar}$ and $|\Delta y|_\ttbar $. To evaluate these quantities it is necessary to reconstruct the full \ttbar kinematics.
%While one might first attempt to reconstruct the full \ttbar kinematics to recover these quantities, it is natural to consider other related kinematic variables as well. 
Figure \ref{fig:ttdia} shows a diagram of \ttbar decay in the dilepton channel. Two neutrinos are present in the final state. While it is possible to completely reconstruct the neutrinos momenta under some assumptions, such reconstruction is highly sensitive to the poorly measured MET. 
%In addition, it is also affected by uncertainty on the jet energy, and comes with a separate systematic uncertainty implemented at the recommendation of the JETMET POG \cite{metunc}.  
We found that a more sensitive measurement results from discarding the neutrino kinematics entirely, using only the kinematics of the better-reconstructed b-jets and leptons. 

We introduce the proxy variables
\begin{eqnarray}
    \Mbl &=& M(b+\bar{b}+l+\bar{l}), \\
    \absDybl &=& |y(b+\bar{l}) - y(\bar{b}+l)|,
\end{eqnarray}
which were found to have good sensitivity to EW corrections while being less vulnerable to resolution effects and systematic uncertainties.

\subsection{b-jet pairing}
\label{SS:bjpair}

After object selection, a two-fold ambiguity remains in pairing the b-jets with the leptons that originates from the same top quark (Figure \ref{fig:ttdia}), i.e. $b\leftrightarrow \bar{l}$, $\bar{b}\leftrightarrow l$. The kinematic variable $\Mbl$, unlike $M_\ttbar$, is independent of this pairing. However, \absDybl\, which provides an increase in sensitivity, is. 
%This is because a coarse binning in $|\Delta y|_{bl}$ helps to distinguish the shape of the signal from shapes produced by systematic uncertainty templates. 
%However, we also benefit from the fact that $\Mbl$, unlike $M_\ttbar$, is independent of this pairing. 

In order to make this pairing, we utilize the information from the kinematic constraints governing the neutrino momenta, but do not perform a full reconstruction.
If one assumes the top quarks and $W$ bosons to be on-shell, the neutrinos momenta are constrained by a set of quadratic equations arising from the conservation of 4-momentum at each vertex. We refer to these kinematic equations, collectively, as the mass constraint. The mass constraint results  in a continuum of possible solutions for neutrino momenta, which geometrically can be presented as  an intersection of ellipsoids in 3D momentum-space (see \cite{burt} for details). For certain values of input momenta of $b$-jets and leptons, these ellipsoids do not intersect at all, and thus the mass constraint cannot be satisfied.

If the mass constraint is satisfied, one can also apply the MET constraint, equating the total missing $p_T$ in the event to the sum of the $p_T$ of the two neutrinos. This constraint reduces the set of possible solutions to a discrete set of 2-4. Just like in the case of the mass constraint, there some values of the input parameters, for which the MET constraint cannot be satisfied. 

%In practice, one can also find an approximate solution as long as the mass constraint can be satisfied \cite{burt}.
This information is useful in picking the correct b-jet pairing (see Figure \ref{fig:bars}). Pairings with no solution to the mass constraint are generally incorrect. When the mass constraint can be satisfied, pairings with a solution to the MET constraint are more likely to be correct. This information is used as part of the pairing procedure.

The full procedure for b-jet pairing is as follows:
\begin{enumerate}

    \item The mass constraint is checked for both possible pairings. If only one pairing is found to satisfy the mass constraint, that pairing is used. If both pairings fail to satisfy the mass constraint, the event is discarded. If both pairings satisfy the mass constraint, we check the MET constraint.
    \item If only one pairing allows for the MET constraint while the other does not, the pairing yielding an exact solution to the MET constraint is used.
    \item If the neutrino kinematics do not suggest a clear pairing, the $b$-jets, $b_1$ and $b_2$, are paired with the leptons ($l$, $\bar{l}$) by minimizing the quantity $$ \Sigma_{1(2)} =\Delta R(b_{1(2)},l)+ \Delta R(b_{2(1)},\bar{l})$$ among the two possible pairings. This method will choose the correct pairing with 74\% accuracy if both $b$-jets have been identified correctly. 

\end{enumerate}
In the end, the correct $b$-jet pairing is selected for approximately 82\% of events, where both $b$-jets are correctly identified.

\begin{figure}
    \centering
    \includegraphics[width=.48\linewidth]{figs/bars_right.pdf} 
    \includegraphics[width=.48\linewidth]{figs/bars_swap.pdf}
    \caption{Distribution over the possible outcomes of the neutrino kinematic constraints assuming correct (left) and swapped (right) b-jet assignment. 
   % These graphs are generated from the same set of events, where both b-jets were correctly identified and reconstructed. We see that, although the kinematic solutions following these constraints are often of poor quality, the possibility to satisfy the kinematic constraints is useful in pairing b-jets with the correct lepton.
    }
    \label{fig:bars}
\end{figure}

\subsection{Sensitivity after reconstruction}
\label{SS:reso}


The sensitivity of our chosen kinematic variables to $\yt$, before and after reconstruction, is shown in Figures \ref{fig:ratblgen} and \ref{fig:ratblrec}. The effect of $\yt$ remains substantial enough, and unique enough in shape, that a measurement can be extracted.

\begin{figure}
    \centering
    \includegraphics[width=.48\linewidth]{figs/ratMblgen.pdf}
    \includegraphics[width=.48\linewidth]{figs/ratDyblgen.pdf}
    \includegraphics[width=.48\linewidth]{figs/ratMblgen_rel.pdf}
    \includegraphics[width=.48\linewidth]{figs/ratDyblgen_rel.pdf}
    \caption{Effect of the EW corrections on kinematic distributions which do not require full kinematic reconstruction of the \ttbar system. The effect is shown both relative to the NLO QCD POWHEG+P8 Monte Carlo Sample (top left \& right), and relative to the Standard Model EW corrections (bottom left \& right). In essence, the top plots show the effect of the EW corrections on the uncorrected sample, while the bottom plots show our sensitivity in distinguishing the effects of different Yukawa coupling values.}
    \label{fig:ratblgen}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.48\linewidth]{figs/ratMblrec_rel.pdf}
    \includegraphics[width=.48\linewidth]{figs/ratDyblrec_rel.pdf}
    \caption{Effect of the EW corrections on \ttbar differential kinematic distributions for different values of $\yt$. We see that the sensitivity to our parameter of interest $\yt$ is not greatly reduced by the reconstruction process, relative to Figure \ref{fig:ratblgen}.}
    \label{fig:ratblrec}
\end{figure}



\subsection{Comparison of data and Monte Carlo prediction}
\label{SS:control}
The comparison between data and Monte Carlo is shown in Figures \ref{fig:dmcpre_16}-\ref{fig:dmcpost_18}. The agreement is generally seen to be well within known uncertainty. We do see some slopes in the ratio of data to MC prediction in the distributions of lepton and $b$-jet $p_T$, to varying extent for each year. These are most likely related to a well known disagreement in top quark $p_T$ distribution. 

It is worth noting that the uncertainty displayed in Figures \ref{fig:dmcpre_16}-\ref{fig:dmcpost_18} can be a bit misleading. Firstly, our parameter of interest is sensitive mainly to shape uncertainties and not to normalization uncertainties. We also include here the uncertainty of the scale variations, which effect the normalization by $\approx 10\%$. However, the $\ttbar$ sample is normalized to the NNLO cross section value, which should better constrain the total cross section normalization, if not the shape. This is accounted for later in the measurement itself, but not reflected in the control plots. 


\begin{figure}[h]
    \centering
    \includegraphics[width=.44\linewidth]{control/16_all_Njets.pdf}
    \includegraphics[width=.44\linewidth]{control/16_all_MET.pdf}
    \includegraphics[width=.44\linewidth]{control/16_all_lep_pt.pdf}
    \includegraphics[width=.44\linewidth]{control/16_all_bj_Pt.pdf}
    \caption{2016 Data to MC comparison.}
    \label{fig:dmcpre_16}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=.44\linewidth]{figs/toppt_16.pdf}
%     \includegraphics[width=.44\linewidth]{figs/nupt_16.pdf}
%     \includegraphics[width=.44\linewidth]{figs/leppt_16.pdf}
%     \includegraphics[width=.44\linewidth]{figs/bjpt_16.pdf}
%     \caption{2016 Data/MC agreement after neutrino/top reconstruction}
%     \label{fig:dmcpost_16}
% \end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.44\linewidth]{control/17_all_Njets.pdf}
    \includegraphics[width=.44\linewidth]{control/17_all_MET.pdf}
    \includegraphics[width=.44\linewidth]{control/17_all_lep_pt.pdf}
    \includegraphics[width=.44\linewidth]{control/17_all_bj_Pt.pdf}
    \caption{2017 Data to MC comparison. }
    \label{fig:dmcpre_17}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=.44\linewidth]{figs/toppt_17.pdf}
%     \includegraphics[width=.44\linewidth]{figs/nupt_17.pdf}
%     \includegraphics[width=.44\linewidth]{figs/leppt_17.pdf}
%     \includegraphics[width=.44\linewidth]{figs/bjpt_17.pdf}
%     \caption{2017 Data/MC agreement after neutrino/top reconstruction}
%     \label{fig:dmcpost_17}
% \end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.44\linewidth]{control/18_all_Njets.pdf}
    \includegraphics[width=.44\linewidth]{control/18_all_MET.pdf}
    \includegraphics[width=.44\linewidth]{control/18_all_lep_pt.pdf}
    \includegraphics[width=.44\linewidth]{control/18_all_bj_Pt.pdf}
    \caption{2018 Data to MC comparison. }
    \label{fig:dmcpost_18}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=.44\linewidth]{figs/toppt_18.pdf}
%     \includegraphics[width=.44\linewidth]{figs/nupt_18.pdf}
%     \includegraphics[width=.44\linewidth]{figs/leppt_18.pdf}
%     \includegraphics[width=.44\linewidth]{figs/bjpt_18.pdf}
%     \caption{2018 Data/MC agreement after neutrino/top reconstruction}
%     \label{fig:dmcpost_18}
% \end{figure}


\clearpage
\section{Measurement Strategy and Statistical Methods}
\label{S:stats}

%The first order electroweak corrections, included as interference terms, introduce a correction factor to the \ttbar differential cross section. This correction factor, $R _\mathrm{EW}$, is the ratio of the yield including LO Electroweak corrections to the LO QCD-based yield. It can also be expressed in terms of the marginal effect of electroweak corrections as $R _\mathrm{EW} = 1 + \delta _\mathrm{EW}$.

\subsection{Definitions involving EW corrections}
\label{SS:kfactor}

The standard jargon of perturbation theory can sometimes be confusing, when applied to the EW corrections, since perturbative expansions are done in orders of two separate coupling constants, $\alpha_s$ and $\alpha$. To clarify, we will utilize  notation similar to \cite{Czakon:2017NLOEW}, where a generic observable $\Sigma ^{t\bar{t}}$ is expanded in a series as

\begin{equation}
    \Sigma^{t\bar{t}}(\alpha_s,\alpha)=\sum_{m+n\geq 2} \alpha_s^m \alpha^n \Sigma _{m,n}
\end{equation}

In practice, contributions due to terms with $m<2$ or $n>2$ are small, and as such often neglected in the calculation of the differential cross sections. In this analysis the following contributions are of interest:

\begin{eqnarray}
    \Sigma_\mathrm{LO\; QCD} &=& \alpha_s ^2 \Sigma_{2,0}\\
    \Sigma_\mathrm{NLO\; QCD} &=& \alpha_s ^3 \Sigma_{3,0}\\
    \Sigma_\mathrm{NLO\; EW} &=& \alpha_s ^2 \alpha \Sigma_{2,1}\\
    \Sigma_\mathrm{extra\; EW} &=& \alpha_s ^3 \alpha \Sigma_{3,1}
\end{eqnarray}

In using the \textsc{HATHOR} package to compute the differential cross section, as a function of \yt, we include the terms

\begin{equation}
    \Sigma_\textsc{HATHOR}(\yt) = \Sigma_\mathrm{LO\; QCD} + \Sigma_\mathrm{NLO\; EW}(\yt).
\end{equation}
The EW corrections K-factor can be defined at the LO QCD level:

\begin{equation}
    K^{\mathrm{NLO}}_\mathrm{EW} = \frac{ \Sigma_\mathrm{LO\; QCD} + \Sigma_\mathrm{NLO\; EW}}{\Sigma_\mathrm{LO\; QCD}}
\end{equation}

Meanwhile, the Powheg+Pythia samples include terms
\begin{equation}
    \Sigma_\textsc{POWHEG} = \left( \Sigma_\mathrm{LO\; QCD} + \Sigma_\mathrm{NLO\; QCD} \right)_{+\mathrm{PS}},
\end{equation}
Where the subscript ${+\mathrm{PS}}$ indicates that parton shower effects have been included as well. Potential variation of these parton shower effects should be covered by the standard procedure of evaluating such uncertainties. 
%While it would be nice to have a sample including EW terms with PS effects, these effects should be smaller than the fairly large uncertainty we will soon introduce on our EW corrections, so we won't devote much further discussion to them.

Since $\alpha \ll \alpha_s $, it often makes some sense to 
%choose some $N$ and 
work at fixed order $m\leq N$, $n\leq 1$, rather than at  $m+n\leq N$ (as mentioned in \cite{pagani2016photon}). Thus,  the EW corrections to the NLO QCD calculations are frequently included using the K-factor, defined at the LO QCD level:
\begin{equation}
    \Sigma_\mathrm{NLO\; QCD\times EW} \equiv K^{\mathrm{NLO}}_\mathrm{EW}\times \left( \Sigma_\mathrm{LO\; QCD} + \Sigma_\mathrm{NLO\; QCD} \right) 
    \label{eq:multiplicative1}
\end{equation}

We will refer to this as the ''multiplicative approximation", which is similar to the ``multiplicative approach" referenced in \cite{Czakon:2017NLOEW}, aside from the missing NNLO QCD terms, which are  covered by the uncertainty due to scale variations. This is the best approximation for EW corrections available to us, because in many kinematic regimes these corrections factorize from their QCD counterparts. Thus, this approach allows us to approximate the terms in $\Sigma_\mathrm{extra\; EW} $ in addition to those directly generated in HATHOR. Specific to this analysis are the dependence on \yt and the use of Powheg, so we can write out our formula for observables more explicitly as 

\begin{equation}
    \Sigma_\mathrm{NLO\; QCD\times EW}^{\yt} \equiv K^{\mathrm{NLO}}_\mathrm{EW} (\yt)\times \Sigma_\textsc{POWHEG}
    \label{eq:multiplicative2}
\end{equation}

In order to estimate the uncertainty on this approach to including EW corrections, we investigate also the more naive additive approach,
\begin{equation}
\Sigma_\mathrm{NLO\; QCD+ EW}^{\yt} \equiv \left(K^{\mathrm{NLO}}_\mathrm{EW}\times \Sigma_\mathrm{LO\; QCD}\right) + \Sigma_\mathrm{NLO\; QCD},
\end{equation}

which neglects  the term  $\Sigma_\mathrm{extra\; EW} $ entirely. We take the difference between the two approaches as an uncertainty of the method:
\begin{eqnarray}
\mathrm{EW\;unc.} &=& \Sigma_\mathrm{NLO\; QCD\times EW}^{\yt} - \Sigma_\mathrm{NLO\; QCD+ EW}^{\yt}\label{eq:uncline1} \\ 
&=& \left(\Sigma_\textsc{POWHEG} - \Sigma_\mathrm{LO\; QCD} \right)(K^{\mathrm{NLO}}_\mathrm{EW}(\yt) - 1 )\label{eq:uncline2} \\
 &\equiv& (\Sigma_\textsc{POWHEG}\times \delta_\mathrm{QCD}) \times (\delta_\mathrm{EW}),
\label{eq:uncline3}
\end{eqnarray}
where we have introduced the shorthand 
\begin{equation}
\delta_\mathrm{QCD}=\frac{\Sigma_\textsc{POWHEG} - \Sigma_\mathrm{LO\; QCD} }{\Sigma_\textsc{POWHEG}}, \;\;\;\; \delta_\mathrm{EW} =(K^{\mathrm{NLO}}_\mathrm{EW}(\yt) - 1 ),
\end{equation}
to illustrate that this uncertainty is effectively a cross term which arises from the difference in additive and multiplicative approaches. Note that $\delta_\mathrm{EW}$ represents the usual marginal effect in terms of which EW corrections are often understood.

The implementation of this uncertainty is detailed in Section \ref{SS:statmeth}. Though this is almost certainly an overestimation, it is difficult to ascribe a more accurate uncertainty without actually computing the missing terms in $\Sigma_\mathrm{extra\; EW} $. 
%This may be possible with the Sherpa generator, but varying the top Yukawa coupling is often outside of the intended use of Monte Carlo generators, and may require some coordination with the Sherpa authors.

\subsection{Implementation of EW corrections}

Events are reweighted with the additional weight factor of $w_\mathrm{EW}(\yt)$ to account for the effects of 1-loop EW corrections which enter at $\mathcal{O}(\alpha_\mathrm{S}^2\alpha)$. We use the HATHOR package to generate the weights $w_\mathrm{EW}$ as a 2-dimensional function of $M_{\ttb}$ and $\Delta y $. This weight correction is generated and applied to partons at generator level in our \ttbar MC samples for a set of discrete values $\yt=0,1,...,5$. Reconstruction is carried out and the resulting double-differential distribution of events in $M_{\mathrm{bl}}$ and $\Delta y_{\mathrm{bl}}$ is stored for each $\yt$ value.

\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{figs/ttbinned.png}
    \caption{\ttbar MC  events after reconstruction and binning, 2017. The dashed line divides the two \absDybl bins, while the horizontal axis shows the bins' \Mbl ranges}
    \label{fig:ttbinned}
\end{figure}

After reconstruction, events are binned coarsely in $|\Delta y_{\mathrm{bl}}|$ and more finely in $M_{\mathrm{bl}}$. The binning is chosen such that no year will have fewer than 5,000 signal events in any given bin \ref{fig:ttbinned}. In each bin, the yield is examined as a function of $\yt$.  The effect is almost exactly quadratic as a consequence of the main diagrams involved (Figure \ref{fig:diagrams}), so we perform a quadratic fit to extrapolate the effect of the EW correction on a given bin as a continuous function of $\yt$ (figure \ref{fig:quad}). This correction for each bin can be applied as a rate parameter $R_\mathrm{EW}^\mathrm{bin}(\yt) = 1 +  \delta _\mathrm{EW}(\yt)$ affecting the expected bin content. 

\subsection{Statistical treatment in Combine}
\label{SS:statmeth}

Using this approach, we use the Combine statistics package to obtain the best  fit for the parameter of interest (POI), $\yt$. Combine maximizes a likelihood function $\mathcal{L}$, which is defined as a product over bins, 

\begin{equation}
    \mathcal{L} = \prod_{\mathrm{bin}\in (\Mbl,|\Delta y|_{bl}) } \mathcal{L}_\mathrm{bin}\;,
\end{equation}
where  the likelihood in each bin is a product of the Poisson probability for a statistical fluctuation and a penalty term for nuisance parameters, which deviate from their nominal/expected values:
% \begin{equation}
%     \mathcal{L}_\mathrm{bin} = \mathrm{Poisson}\Big[n^\mathrm{bin}_\mathrm{observed}  \Big\vert n^\mathrm{bin}_\mathrm{expected} (\mathrm{nuisances})\Big] \times \rho(\mathrm{nuisances}\vert\mathrm{expected})
% \end{equation}
% Filling in the details for this analysis, we have
\begin{equation}
    \mathcal{L}_\mathrm{bin} = \mathrm{Poisson}\Big[n^\mathrm{bin}_\mathrm{obs}  \Big\vert s^\mathrm{bin}(\{\theta_i \}) \times R_\mathrm{EW}^\mathrm{bin}(\yt, \phi) +b^\mathrm{bin}(\{\theta_i\}) \} \Big] \times p(\phi)\times \prod_i p(\theta_i).
\end{equation}

Here $n^\mathrm{bin}_\mathrm{obs}$ is the total observed bin count, with the expected bin count being the sum of  the predicted signal $s^\mathrm{bin}$ and background $b^\mathrm{bin}$. The number of expected signal events is modified by the additional rate $R_\mathrm{EW}$, which depends on the Yukawa coupling ratio $\yt$ and the special nuisance $\phi$, discussed further below.

\begin{figure}
\centering
\includegraphics[width=.48\linewidth]{quadplots/2017bin1.pdf}
\includegraphics[width=.48\linewidth]{quadplots/2017bin16.pdf}
 \caption{The EW correction weights in two separate [$\Mbl, \DYbl$] bins, demonstrating the quadratic dependence on $y_\mathrm{t}$. }
    \label{fig:quad}
\end{figure}
%The parameter $\phi$ is separated  from the other parameters due to its special interaction with the EW corrections from which the measurement is derived.
Each of the other nuisance parameters, $\{\theta_i\}$, that affect the expected counts in each bin, is described by a gaussian probability distribution function of $p(\theta_i)$ that describes the likelihood of the deviation of $\theta_i$ from its expected value.  The correlations between nuisance parameters are discussed in Appendix~\ref{SS:corr}. 

We include an estimate of the uncertainty on the multiplicative application of EW corrections derived at order $\mathcal{O}(\alpha_\mathrm{S}^2\alpha)$, as discussed in Section \ref{SS:kfactor}. The full expression for the rate $R_\mathrm{EW}^\mathrm{bin}$, including this uncertainty term, is
\begin{equation}
    R_\mathrm{EW}^\mathrm{bin}(\yt, \phi) = (1+ \delta_\mathrm{EW}^\mathrm{bin}(\yt))\times(1+\delta_\mathrm{QCD}^\mathrm{bin}\delta_\mathrm{EW}^\mathrm{bin}(\yt))^\phi,
\end{equation}
where $\delta_\mathrm{QCD}^\mathrm{bin}\delta_\mathrm{EW}^\mathrm{bin}$ represents the cross term arising from the difference in multiplicative and additive approaches, based on Equations \ref{eq:uncline1}. The normally distributed nuisance parameter $\phi$ modulates this uncertainty. The definitions of Section \ref{SS:kfactor} are applied in terms of the expected bin yields $n^\mathrm{bin}$ after reconstruction:

\begin{eqnarray}
\delta_\mathrm{EW} =& (K^{\mathrm{NLO}}_\mathrm{EW}(\yt) - 1 ) \;\longrightarrow\;& \delta_\mathrm{EW}^\mathrm{bin}= \frac{n^{\mathrm{bin}}_\mathrm{\textsc{HATHOR}}-n^\mathrm{bin}_\mathrm{LO}}{n^\mathrm{bin}_\mathrm{LO}} \\
\delta_\mathrm{QCD} =& \frac{\Sigma_\textsc{POWHEG} - \Sigma_\mathrm{LO\; QCD} }{\Sigma_\textsc{POWHEG}} \;\longrightarrow\;& \delta_\mathrm{QCD}^\mathrm{bin}= \frac{n^{\mathrm{bin}}_\mathrm{\textsc{POWHEG}}-n^\mathrm{bin}_\mathrm{LO}}{n^\mathrm{bin}_\mathrm{POWHEG}} 
\end{eqnarray}

%where $\phi$ modulates the uncertainty on the EW corrections, obeying a normal distribution. 
We note that the uncertainty on the EW corrections is unique because it depends on the value of \yt, at which the EW corrections are evaluated. Thus, it is given its own term and nuisance parameter $\phi$ separate from other systematic uncertainties.







\clearpage

\section{Dominant uncertainty sources}
\label{S:mainuncs}

We list here the main limiting systematic uncertainties for this measurement. Many additional uncertainties with minor effects are also included in the fit, which are described in Appendix \ref{S:otherunc}.

\subsection{Experimental uncertainty sources}

\par \noindent
\textbf{Jet Energy Scale (JES\_$\ast$)}

Corrections are applied to Jet Energy Scale (JES) to account for a variety of effects which influence the reconstructed jet energy. This is broken into many separate independent components, the largest of which are significant uncertainties in the measurement.


\par \noindent
\textbf{Jet Energy Resolution (JERup, JERdown)}

The Jet Energy Resolution (JER) is observed to be worse in data than in Monte Carlo. To account for this, a smearing is applied. The uncertainty on this procedure is modeled by shifting the desired final resolution (as a function of $\eta$ and $p_T$) by $\pm 1\sigma$.
As a conservative implementation, the JER is varied up and down to form two separate uncertainty templates, which are then symmetrized separately.

\subsection{Theoretical uncertainty sources}

\noindent \textbf{Renormalization and factorization scales (rs, fs, rsfs)}

The QCD renormalization and factorization scales are each varied up and down in the Monte Carlo by a factor of 2, as a standard technique to estimate uncertainty due to higher-order QCD terms.

This standard practice is meant to account for contributions from higher order QCD terms to the Monte Carlo. While it does a decent job of estimating the potential magnitude of these corrections, it is a rather imprecise way of estimating their possible shape, to which this analysis is sensitive. To help capture a greater variety of possible shape distortions coming from higher order terms, we include a same sign variation, rsfs, in which the two scales are varied simultaneously (same sign variation).

Since we normalize the cross section to the NNLO value, we remove the overall normalization effect from all scale variation uncertainties, which otherwise would be overestimated. 
%If including the normalization component, this would result in an overestimate of total uncertainty. However, we remove , as . From a shape-based perspective, the same sign variation is a perfectly valid candidate for higher order effects. 

\par \noindent
\textbf{Uncertainty due to Electroweak (HATHOR) correction} 

We include an uncertainty on the EW corrections, based on our methods for generating and applying these additional terms, as outlined in Sections \ref{SS:kfactor} and \ref{SS:statmeth}.

Like the scale variations, this uncertainty stands in for higher-order effects, predominantly those coming from diagrams of order $\alpha_s^3 \alpha$. 
%It should be possible in the near future to examine these terms directly, although it is not clear if this will be achievable in the timespan of this analysis due to tight schedule constraints.
\par \noindent
\textbf{Top mass (mtop)}

Dedicated Monte Carlo samples are generated with the top mass varied up and down by 1 \unit{GeV} to estimate the effect of the top mass uncertainty. 
\par \noindent
\textbf{Initial and final state radiation (isr, fsr)}

Uncertainty due to initial and final state radiation is assessed by varying the value of the strong coupling $\alpha_\mathrm{s}$ up and down in the parton shower algorithm, in the initial and final states, respectively. In 2016 this was done using dedicated Monte Carlo samples. The effect in these samples was thought to be unrealistically large, so the effects are scaled down by a factor of $1/\sqrt{2}$ for this year, following the prescription in \cite{Skands:2014pea}. In 2017 and 2018, the uncertainty was implemented via weights in the central Monte Carlo sample. We use the weights corresponding to a variation of $\alpha_\mathrm{s}$ by a factor of 2.
\par \noindent
\textbf{$b$-jet fragmentation (bfrag)} 

The momentum transfer from $b$ quarks to $B$ hadrons is modeled with a transfer function dependent on $x_b=\frac{p_T(B)}{p_T(b-jet)}$. To estimate the uncertainty, the transfer function is varied up and down by 1$\sigma$ of the Bowler-Lund parameter in PYTHIA8. The resulting effect is applied via weights.
\par \noindent
\textbf{Signal and background normalization (tt\_norm, st\_norm, vj\_norm)} 

A 10\% normalization uncertainty is included on the \ttbar sample, meant to represent the normalization effects of scale variations, which have been subtracted from their templates. This is admittedly an overestimation, but we should not be very sensitive to normalization effects, so it is included presently as a check on the fit.

The backgrounds in this analysis are small enough ($\approx$2\% sample composition each) that we do not generate templates for their response to individual systematic uncertainties. Instead, a 15\% normalization uncertainty is included on each.


\subsection{Processing of systematics}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{templates/yuk.pdf}
    \caption{The effect of the yukawa parameter \yt on binned reconstructed events. We see that the effect induces a shape distortion on the kinematic distributions not unlike a systematic uncertainty template.}
    \label{fig:yuktemplate}
\end{figure}

In this analysis, the POI, $\yt$, manifests itself as a smooth shape distortion of the kinematic distributions as seen in Figure \ref{fig:yuktemplate}. This behavior lessens the effect of normalization systematics, but also means we must take extra care in handling the shape systematics. Many sources of systematics are described by templates with significant bin-to-bin variations (see Appendix \ref{SS:templates}). While these variations can be genuine, more frequently they are due to fluctuations in the dedicated Monte Carlo samples with low statistics or other issues involving a lack of available information. These systematic uncertainties will typically be over-constrained by the likelihood fit while having little impact on the POI. As a result their potential impact on the POI could be  vastly underestimated. We thus attempt to smooth out fluctuations in our systematic templates which allow a better estimate of their potential effects on the measurement by assuming that they arise from some smooth distribution like the POI.

Systematic templates are also symmetrized, and partial correlations are enforced on nuisance parameters between different years based on our knowledge of the underlying uncertainty sources. Details of these procedures can be found in Appendix \ref{S:uncdetails}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sensitivity Study (Results)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\label{S:fit}
To study the expected sensitivity of our measurement of $\yt$, we perform a maximum likelihood fit using the Asimov dataset with $\yt=1$. The fit results in a 60\% CI of [.58,1.31], and a 95\% CI of [0,1.56]. The impacts of the leading nuisances are shown in Figure~\ref{fig:impacts}, while a scan of the negative log likelihood as a function of $\yt$ is shown in Figure~\ref{fig:scan} (left). The result is comparable to the measurement obtained in \cite{ytpaper}, and the fit seems well-behaved.

This section will be modified to include results on data after unblinding.



\begin{figure}
    \centering
    \includegraphics[width=.98\linewidth]{figs/impacts1.pdf}
    \caption{Impacts of systematics on the best fit value for $\yt$ using the Asimov data set. Only the leading 34 systematics are shown.}
    \label{fig:impacts}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=.49\linewidth]{figs/myLL.pdf}
    \caption{The result of a likelihood scan, performed by fixing the value of $\yt$ at values over the interval [0,2.5] and minimizing the negative log likelihood (NLL). } 
    \label{fig:scan}
\end{figure}



\clearpage


\section{Summary}
\label{S:conc}

A measurement is presented of the Top-Higgs Yukawa coupling is presented, based on data from proton-proton collisions at the CMS experiment. Data at center of mass energy $\sqrt{s}=13$\,TeV is used from the full LHC run 2, collected from 2016-2018, yielding an integrated luminosity of 136.3 fb$^{-1}$. This measurement uses the effects of virtual Higgs exchange on \ttbar decays to extract information about the coupling from kinematic distributions. Though the sensitivity is lower compared to the direct limit in \cite{ttH}, it avoids a direct dependence on other Yukawa coupling values through additional decays. Perhaps a more analogous measurement of the Top Yukawa is the limit on the four-top cross section, which places 95\% CI upper limit of \yt$ < 2.1$ \cite{fourtop}.

Based on the sensitivity study, we expect this measurement to have comparable sensitivity to the lepton+jets channel measurement in \cite{ytpaper}. This section will be updated when the fit is unblinded.

\clearpage

\appendix

\section{Additional uncertainty sources}
\label{S:uncs}

The list of uncertainties considered is very similar to that of the analogous measurement using 2016 data in \cite{ytpaper}. The main differences are the lack of QCD multijet background and the  use of the full run 2 dataset. Some uncertainty sources are evaluated differently in the 2017-2018 Monte Carlo samples compared to the 2016 sample. In this section we first give a brief overview of each uncertainty source, then we comment on the processing of systematic templates and their correlation between data set corresponding to different years.

\label{S:otherunc}
\label{SS:syslist}
\subsection*{Experimental uncertainty sources}


\noindent \textbf{Luminosity (lumi)}

The overall uncertainty on the integrated Luminosity is included as a scale uncertainty applied to signal and background events. 

\par \noindent
\textbf{Pileup (pu)}

The inelastic cross section, 69.2\unit{mb}, is varied by $1\sigma = 4.6\%$ to estimate the uncertainty from pileup reweighting.

\par \noindent
\textbf{MET}

The missing transverse momentum, or MET, is affected by the uncertainty on jet energy. However, there is also some systematic uncertainty on the MET due to the energy scales of particles which are not reconstructed as part of a jet, which affects both  the MET resolution and the MET scale.

While  this uncertainty was found to be negligible in reconstructing semi-leptonic decays, it is increased in the dilepton channel when attempting a full kinematic reconstruction, due to the nonlinear dependence of neutrino solutions on the MET. Since we do not attempt a full reconstruction, the MET uncertainty now only enters our analysis as a negligible effect on the same-flavor lepton decays, where a MET-based selection is used.

\par \noindent
\textbf{Lepton scale factor (eltrg, elrec, mutrg, murec)}

A scale factor is applied to correct for efficiencies of lepton reconstruction, ID, isolation, and triggers. This scale factor, and it's inefficiencies, are evaluated using the tag-and-probe technique on $Z$ boson decays. As we use single lepton triggers, this uncertainty must be split into two uncertainties for each lepton flavor (mu/el), which we label rec (reconstruction+ID+iso) and trg (trigger).

\par \noindent
\textbf{$b$-tagging and mis-tagging (btag, ltag)}

To account for discrepancies in the b-tagging and mis-tagging efficiencies between data and Monte Carlo, scale factors are applied to each jet. These factors are varied up and down by $1\sigma$ to estimate the corresponding uncertainty.

\par \noindent
\textbf{Prefire }

An uncertainty is included on the weights generated to address a problem affecting the ECAL in 2016 and 2017.

\subsection*{Theoretical uncertainty sources}




\par \noindent
\textbf{PDFs (pdf\_i, pdf\_alpha)}

We use the NNpdf set \cite{NNPDF}, which contains 100 individual variations as uncertainty sources.  As in \cite{ytpaper}, some of these are combined to reduce the number of variations to a more manageable 10 templates. These uncertainties are then subject to the pre-fit processing (Section \ref{SS:processing}), so they do not all appear in the final fit. We also note that these uncertainties are treated as fully correlated between 2017/2018 datasets, but not in the 2016 MC where they were generated differently.
The effect of the value of the strong coupling $\alpha_s$ used by NNpdf is included and treated separately from the other pdf variations.



\par \noindent
\textbf{Matrix element / parton shower matching (hdamp)}

The $h_{damp}$ parameter, which controls the matrix element parton shower matching in \small\textsc{POWHEG + PYTHIA8}, is set to the nominal value of $h_{damp}= 1.58 \cdot m_\mathrm{t}$. Dedicated Monte Carlo samples are generated with this parameter varied down to $1 \cdot m_\mathrm{t}$ and up to $2.24 \cdot m_\mathrm{t}$, in order to estimate the effect of this uncertainty.

\par \noindent
\textbf{Monte Carlo tune / underlying event (tune)}

Dedicated Monte Carlo samples are generated with variations of the PYTHIA tune in order to estimate the effect of modeling the underlying event.





\par \noindent
\textbf{B decay branching (bdec)} 

The b-jet response is sensitive to the branching ratio of semi-leptonic B decays. Samples are reweighted with this branching fraction varied up and down by 1$\sigma$ according to the PDG value of the uncertainty. 






\section{Details of uncertainty treatment}

\label{S:uncdetails}
\subsection{smoothing}
\label{SS:processing}


Smoothing is applied to the \Mbl shape in each \DYbl\, bin after binning and forming the marginal templates. Since the bins are chosen to have similar statistics, this procedure closely mirrors the outcome of a variable-width smoothing done based on statistical uncertainties in a fixed-width binning. 
%Attempts to employ a fixed width binning on the \Mbl spectrum yielded nonsensical results at high \Mbl values where statistics are low.

We employ a weighted moving average smoothing technique. If the input bin values are $\{b_i\}$, the value of a given bin after smoothing is 

\begin{equation}
    B_i = \frac{ \sum_{j=1}^N w_j\cdot b_j}{\sum_{j=1}^N w_j},\quad\quad
    w_i = \exp{\left[\frac{-|i-j|}{2s}\right]}.
    \label{eq:smooth}
\end{equation}
The parameter $s$ determines the width of the smoothing. Rather than assign it arbitrarily, we attempt to calculate a measure of ''roughness" of the template beforehand. To do this, we first smooth the bins $b^\mathrm{in}_i$ with smoothing width $s=2$ to obtain $b^\mathrm{out}_i$. We  define an average magnitude of the template $b$ to as $\mathrm{mag}(b) = \frac{1}{N}\sum_{i=1}^N |b_i|$. We then calculate the average marginal change in the template's bin content, defining the statistic:

\begin{equation}
\zeta = \frac{1}{N}\sum_{i=0}^N  \frac{|b^\mathrm{in}_i-b^\mathrm{out}_i|}{\mathrm{mag}(b)}.
\end{equation}

The parameter $\zeta$ is a measure of the smoothness of a certain systematic and its potential impact on the POI. 
The value of $\zeta$  generated by the POI itself is consistently less than 0.1. 
%On the other hand, some systematics are quite noisy and yield much larger values. These systematics are found to have erratic, often underestimated effects on the fit, meanwhile often typically over-constrained. If large, they can also affect the stability of the fit, by creating a very jagged likelihood function. 
%To remedy this, 
If a systematic uncertainty template with binned values $\{b_i\}$ yields a parameter $\zeta>0.25$, we choose to smooth it using Eq.~\ref{eq:smooth} with a width of $s=4 \zeta$. This value typically results in smooth templates that consistently provide a more stable fit and with a more conservative result.

Since the POI template is asymmetric  it is not surprising that even slightly asymmetric systematics can yield a one-sided impact on $\yt$. This generally ill-regarded phenomenon can be remedied by symmetrizing the systematic templates. After smoothing, the ``down" template is negated and compared to the ``up" template. In each bin, the  value with the largest magnitude is taken to generate a new ``up" templates, which is given a symmetric ``down" template. This is found to yield a slightly more conservative sensitivity in the fit, while helping the convergence of the fit as well as avoiding some one-sided impacts. 

Lastly, systematics which are extremely small ($<0.1\%$ effect in msot bins) or extremely noisy (tune, hdamp) are flattened and included together in an overall normalization uncertainty. 



\subsection{Correlation of systematics across run 2 datasets}
\label{SS:corr}


In our treatment of systematics between the three years, we allow the possibility that systematics are uncorrelated, fully correlated, or partially correlated between years. 

In general, the correlations for experimental uncertainties are known, but are sometimes approximated for ease of implementation, especially when small.
Though theoretical uncertainties are generally correlated year-to-year, we must take into account factors such as changes in modelling and limited statistics in dedicated MC samples. The list of correlation values used in the final fit is shown in Table \ref{tab:run2corr}.
\begin{table}[h]
    \centering
\begin{tabular}{|l|l|l|l|}
\hline
Correlation between years:              & 16-17  & 16-18  & 17-18  \\ \hline \hline
EW correction uncertainty               & 100\%  & 100\%  & 100\%  \\ \hline
background normalization                & 100\%  & 100\%  & 100\%  \\ \hline
renormalization \& factorization scales & 100\%  & 100\%  & 100\%  \\ \hline
b decay, b fragmentation                                 & 100\%  & 100\%  & 100\%  \\ \hline
pdf uncertainties                       & 0\%    & 0\%    & 100\%  \\ \hline
ISR, FSR               & 50\%   & 50\%   & 100\%  \\ \hline
top mass                                & 100\%  & 100\%  & 100\%   \\ \hline
hdamp, MC tune *                        & 0\%    & 0\%    & 0\%    \\ \hline \hline
btag SFs, lepton SFs                    & 50\%   & 50\%   & 50\%   \\ \hline
lumi\^                                  & 30\%   & 30\%   & 30\%   \\ \hline
pileup\^                                & 50\%   & 50\%   & 50\%   \\ \hline
JER                                     & 0\%    & 0\%    & 0\%    \\ \hline
JES                                     & \multicolumn{3}{c|}{varies by component} \\ \hline
\end{tabular}
    \caption{Prescription of systematic uncertainties correlations between run 2 datasets. * Indicates sources for which correlations are motivated by limited statistics. \^\ Indicates sources for which the ideal values have been  slightly modified to allow for an easier implementation in the Combine statistics software package.}
    \label{tab:run2corr}
\end{table}

\subsection{Example templates}
\label{SS:templates}

Some notable examples of systematic uncertainty templates, demonstrating their effect on the binned data, are shown.



\begin{figure} \centering
\includegraphics[width=.35\linewidth]{templates/JES_FlavorQCD_16}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/JES_FlavorQCD_17}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/JES_FlavorQCD_18}
\caption{JES\_FlavorQCD templates. This is by far the dominant component of the JES uncertainty.}
\label{fig:JES-FlavorQCD_template}
\end{figure}


\begin{figure} \centering
\includegraphics[width=.35\linewidth]{templates/fs_16}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/fs_17}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/fs_18}

\includegraphics[width=.35\linewidth]{templates/rs_16}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/rs_17}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/rs_18}

\includegraphics[width=.35\linewidth]{templates/rsfs_16}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/rsfs_17}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/rsfs_18}
\caption{Scale variation templates (rs, fs, rsfs). These show the theoretical uncertainty used to estimate higher-order QCD contributions. Note theat the normalization effects have been removed as described in Section \ref{SS:syslist}}
\label{fig:fs_template}
\end{figure}

\begin{figure} \centering
\includegraphics[width=.35\linewidth]{templates/isr_16}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/isr_17}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/isr_18}

\includegraphics[width=.35\linewidth]{templates/fsr_16}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/fsr_17}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/fsr_18}
\caption{isr and fsr templates. The change in modelling is evident between 2016 and the other years}
\label{fig:fsr_template}
\end{figure}

\begin{figure} \centering
\includegraphics[width=.35\linewidth]{templates/tune_16}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/tune_17}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/tune_18}

\includegraphics[width=.35\linewidth]{templates/hd_16}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/hd_17}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/hd_18}
\caption{tune and hd templates. These come from dedicated MC samples with limited statistics, and do not provide reliable shape templates. They are thus flattened.}
\label{fig:hd_template}
\end{figure}



\begin{figure} \centering
\includegraphics[width=.32\linewidth]{templates/mtop_16_old}
\includegraphics[width=.32\linewidth]{templates/mtop_17_old}
\includegraphics[width=.32\linewidth]{templates/mtop_18_old}
\caption{mtop original templates. One can see common features, but the templates are too noisy for the smoothing to yield a reasonable shape. These are not included in the fit}
\label{fig:mtop_template_old}
\end{figure}

\begin{figure} \centering
\includegraphics[width=.32\linewidth]{templates/mtop_16}
\includegraphics[width=.32\linewidth]{templates/mtop_17}
\includegraphics[width=.32\linewidth]{templates/mtop_18}
\caption{mtop templates after combining all 3 years to make a single template. After combining and smoothing, we see a reasonable shape effect which captures the features shared between the 3 templates in Fig. \ref{fig:mtop_template_old}. Thus, one shape is used for all 3 years.}

\label{fig:mtop_template}
\end{figure}

\begin{figure} \centering
\includegraphics[width=.35\linewidth]{templates/murec_16}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/murec_17}\hskip-.5cm
\includegraphics[width=.35\linewidth]{templates/murec_18}
\caption{murec templates}
\label{fig:murec_template}
\end{figure}






\clearpage
